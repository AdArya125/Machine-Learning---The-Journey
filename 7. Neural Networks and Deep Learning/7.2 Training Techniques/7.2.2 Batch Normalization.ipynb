{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ab44f6-ed71-4d46-9fa3-13a309357945",
   "metadata": {},
   "source": [
    "# 7.2.2 Batch Normalization\n",
    "\n",
    "## Explanation of Batch Normalization\n",
    "\n",
    "Batch Normalization (BN) is a technique designed to improve the training of deep neural networks. It was introduced by Sergey Ioffe and Christian Szegedy in their 2015 paper, \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.\" BN addresses issues related to training stability and speed by normalizing the activations of each layer. \n",
    "\n",
    "\n",
    "## Key Terminologies\n",
    "\n",
    "- **Activation**: The output of a neuron in a neural network after applying an activation function.\n",
    "- **Mean**: The average value of the activations for a given layer.\n",
    "- **Variance**: The measure of the spread of activations around the mean for a given layer.\n",
    "- **Normalization**: The process of adjusting the activations to have a mean of zero and a variance of one.\n",
    "\n",
    "## Process of Batch Normalization\n",
    "\n",
    "Batch Normalization involves normalizing the activations of a layer. The process can be divided into two main steps: the forward pass and the backward pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a16f33-ef1c-4925-b879-7e5d96ff6bbd",
   "metadata": {},
   "source": [
    "\n",
    "### Forward Pass\n",
    "\n",
    "1. **Compute the Mean and Variance**:\n",
    "   \n",
    "   For a given batch of data $X$ (where $X$ has $N$ examples and $D$ features), calculate the mean $\\mu$ and variance $\\sigma^2$ for each feature:\n",
    "\n",
    "   $$\n",
    "   \\mu = \\frac{1}{N} \\sum_{i=1}^{N} X_i\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\mu)^2\n",
    "   $$\n",
    "\n",
    "2. **Normalize the Data**:\n",
    "\n",
    "   Normalize the activations using the computed mean and variance:\n",
    "\n",
    "   $$\n",
    "   \\hat{X} = \\frac{X - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "   $$\n",
    "\n",
    "   Here, $\\epsilon$ is a small constant added for numerical stability.\n",
    "\n",
    "3. **Scale and Shift**:\n",
    "\n",
    "   Apply learned parameters $\\gamma$ (scale) and $\\beta$ (shift) to the normalized data:\n",
    "\n",
    "   $$\n",
    "   Y = \\gamma \\hat{X} + \\beta\n",
    "   $$\n",
    "\n",
    "   Here, $Y$ is the output of the batch normalization layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d329e220-828e-4b84-a814-980420c57e1b",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "\n",
    "1. **Compute Gradients**:\n",
    "\n",
    "   During backpropagation, compute the gradients of the loss with respect to the batch normalization parameters ($\\gamma$, $\\beta$) and the input $X$. The gradients are computed using the chain rule of calculus and involve the derivatives of the mean and variance.\n",
    "\n",
    "   - Gradient with respect to $\\gamma$:\n",
    "\n",
    "     $$\n",
    "     d\\gamma = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial Y_i} \\cdot \\hat{X}_i\n",
    "     $$\n",
    "\n",
    "   - Gradient with respect to $\\beta$:\n",
    "\n",
    "     $$\n",
    "     d\\beta = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial Y_i}\n",
    "     $$\n",
    "\n",
    "   - Gradient with respect to the normalized input $\\hat{X}$ and original input $X$ involve more complex terms considering the effects of normalization on the gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee4d14-19e9-49bd-82a5-e22d1e0e3854",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### Readings:\n",
    "- [Batch Norm Explained Visually](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)\n",
    "- [Batch Normalization](https://medium.com/nerd-for-tech/batch-normalization-51e32053f20)\n",
    "- [Introduction to Batch Normalization](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/)\n",
    "- [Batch normalization in 3 levels of understanding](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb4b4b0-c6f4-48b8-8690-e9ca7bf13794",
   "metadata": {},
   "source": [
    "## Benefits of Batch Normalization\n",
    "\n",
    "- **Improved Training Speed**: BN can reduce the number of training epochs required for convergence.\n",
    "- **Reduced Internal Covariate Shift**: Normalization mitigates the problem of internal covariate shift by stabilizing the distribution of layer inputs.\n",
    "- **Increased Model Stability**: By normalizing activations, BN helps stabilize the training process and can reduce the dependence on careful initialization and learning rates.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "Batch Normalization is widely used in various types of neural networks, including:\n",
    "- Convolutional Neural Networks (CNNs)\n",
    "- Deep Feedforward Networks\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "\n",
    "By normalizing the activations, BN facilitates the training of deeper and more complex neural networks, leading to improved performance and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa3b1a7-3101-4a58-a100-35ee454960cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output:\n",
      " [[ 0.79835038 -0.10633531  0.7310406 ]\n",
      " [ 1.50500184  1.9398276  -1.5772949 ]\n",
      " [-0.40789861 -0.87537424 -0.54579944]\n",
      " [-1.20739247 -0.46346352  1.29223008]\n",
      " [-0.68806115 -0.49465453  0.09982365]]\n",
      "\n",
      "Backward pass gradients:\n",
      "\n",
      "d_X:\n",
      " [[-1.62886563e-03  1.56315931e+00 -5.47883128e-01]\n",
      " [ 4.16198431e-01 -1.95247371e-01 -8.73595579e-01]\n",
      " [-2.92845494e-01  1.15427271e-01  1.70826155e-01]\n",
      " [ 1.59490467e+00 -2.81277254e+00 -8.46188693e-01]\n",
      " [-1.71662874e+00  1.32943333e+00  2.09684124e+00]]\n",
      "d_gamma:\n",
      " [-2.14074433 -2.65652776  4.48771932]\n",
      "d_beta:\n",
      " [ 3.3829314   1.58283307 -1.98519581]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchNormalization:\n",
    "    def __init__(self, input_dim, epsilon=1e-8, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones(input_dim)\n",
    "        self.beta = np.zeros(input_dim)\n",
    "        self.running_mean = np.zeros(input_dim)\n",
    "        self.running_var = np.ones(input_dim)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.var = np.var(X, axis=0)\n",
    "            self.X_hat = (X - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "            self.out = self.gamma * self.X_hat + self.beta\n",
    "            \n",
    "            # Store input for backward pass\n",
    "            self.X = X\n",
    "            \n",
    "            # Update running mean and variance\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "            \n",
    "            return self.out\n",
    "        else:\n",
    "            # During inference, use running mean and variance\n",
    "            X_hat = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            return self.gamma * X_hat + self.beta\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # Compute gradients\n",
    "        N, D = d_out.shape\n",
    "        d_X_hat = d_out * self.gamma\n",
    "        d_var = np.sum(d_X_hat * (self.X - self.mean) * -0.5 * np.power(self.var + self.epsilon, -1.5), axis=0)\n",
    "        d_mean = np.sum(d_X_hat * -1 / np.sqrt(self.var + self.epsilon), axis=0) + d_var * np.mean(-2 * (self.X - self.mean), axis=0)\n",
    "        d_X = d_X_hat / np.sqrt(self.var + self.epsilon) + d_var * 2 * (self.X - self.mean) / N + d_mean / N\n",
    "        \n",
    "        d_gamma = np.sum(d_out * self.X_hat, axis=0)\n",
    "        d_beta = np.sum(d_out, axis=0)\n",
    "        \n",
    "        return d_X, d_gamma, d_beta\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(5, 3)  # Sample data: 5 samples, 3 features\n",
    "\n",
    "batch_norm = BatchNormalization(input_dim=3)\n",
    "\n",
    "# Forward pass\n",
    "out = batch_norm.forward(X, training=True)\n",
    "print(\"Forward pass output:\\n\", out)\n",
    "\n",
    "# Backward pass (example gradients from subsequent layers)\n",
    "d_out = np.random.randn(*X.shape)\n",
    "d_X, d_gamma, d_beta = batch_norm.backward(d_out)\n",
    "print(\"\\nBackward pass gradients:\\n\")\n",
    "print(\"d_X:\\n\", d_X)\n",
    "print(\"d_gamma:\\n\", d_gamma)\n",
    "print(\"d_beta:\\n\", d_beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83075d-4645-4797-94e9-e880e0ef2bd9",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "## Usage in `TensorFlow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e483d8f7-ddcd-473a-9788-96884070d294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4666 - loss: 0.7710\n",
      "Epoch 2/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4902 - loss: 0.7623 \n",
      "Epoch 3/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4973 - loss: 0.7299 \n",
      "Epoch 4/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6570 - loss: 0.6437 \n",
      "Epoch 5/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6604 - loss: 0.6404 \n",
      "Epoch 6/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6591 - loss: 0.6252 \n",
      "Epoch 7/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7294 - loss: 0.5929 \n",
      "Epoch 8/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6679 - loss: 0.5834 \n",
      "Epoch 9/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6997 - loss: 0.5692 \n",
      "Epoch 10/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7178 - loss: 0.5597 \n",
      "Epoch 11/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7897 - loss: 0.5497 \n",
      "Epoch 12/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7955 - loss: 0.5129 \n",
      "Epoch 13/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7093 - loss: 0.5471 \n",
      "Epoch 14/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7563 - loss: 0.5239 \n",
      "Epoch 15/15\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8518 - loss: 0.4995 \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8567 - loss: 0.5502  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization,Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X_train = np.random.randn(100, 10)  \n",
    "y_train = np.random.randint(0, 2, size=(100,))  # Binary classification\n",
    "\n",
    "# Define a simple neural network with Batch Normalization\n",
    "model = Sequential([\n",
    "    Input(shape=(10,)),  # Specify input shape using Input layer\n",
    "    Dense(64),  # Dense layer\n",
    "    BatchNormalization(),  # Batch Normalization layer\n",
    "    tf.keras.layers.Activation('relu'),  # Activation function\n",
    "    Dense(32),  # Hidden layer\n",
    "    BatchNormalization(),  # Batch Normalization layer\n",
    "    tf.keras.layers.Activation('relu'),  # Activation function\n",
    "    Dense(1, activation='sigmoid')  # Output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=16)\n",
    "loss, accuracy = model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2214554f-b2bd-4864-9b9c-715ad2d04eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5401\n",
      "Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300964d-7e5b-4081-b17d-6b2398edeba0",
   "metadata": {},
   "source": [
    "## Conclusion\r\n",
    "\r\n",
    "In this implementation, we explored Batch Normalization within the context of neural networks using TensorFlow. Batch Normalization is a technique that normalizes the activations of each layer to improve the training process and enhance model performance. This technique can speed up training, reduce sensitivity to initialization, and potentially lead to better generalization.\r\n",
    "\r\n",
    "We demonstrated how to integrate Batch Normalization into a neural network model using TensorFlow's Keras API. By placing `BatchNormalization` layers after `Dense` layers, we effectively standardize the outputs, which helps stabilize and accelerate the training process. \r\n",
    "\r\n",
    "The code example provided:\r\n",
    "- Generates synthetic training data.\r\n",
    "- Constructs a simple feedforward neural network with Batch Normalization layers.\r\n",
    "- Compiles, trains, and evaluates the model, showing how to incorporate this technique into a typical workflow.\r\n",
    "\r\n",
    "In practice, incorporating Batch Normalization can be highly beneficial for deep learning models, especially when dealing with complex and large datasets. The use of `Input` layers to define the input shape, as demonstrated, also helps avoid common issues and warnings related to layer specifications.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
