{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57860038-4d5e-46f1-8c12-b0a2bd433624",
   "metadata": {},
   "source": [
    "# 7.2.3 Dropout\n",
    "\n",
    "## Explanation of Dropout\n",
    "\n",
    "Dropout is a regularization technique used in neural networks to prevent overfitting. During training, dropout randomly sets a fraction of the input units to zero at each update during forward pass. This prevents the network from becoming too reliant on any one neuron, thereby forcing it to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
    "\n",
    "Mathematically, dropout can be described as follows:\n",
    "\n",
    "Given a neural network layer's output $a$, dropout is applied as:\n",
    "\n",
    "$$\n",
    "\\tilde{a} = a \\cdot \\text{mask}\n",
    "$$\n",
    "\n",
    "where $\\text{mask}$ is a binary vector with the same shape as $a$, where each element is 0 with probability $p$ and 1 with probability $1-p$. During training, the mask is randomly generated, and $p$ is the dropout rate.\n",
    "\n",
    "## Scenarios Where Dropout is Beneficial\n",
    "\n",
    "1. **Overfitting**: Dropout is especially useful when a model has too many parameters compared to the amount of training data, which can lead to overfitting. By randomly dropping units, dropout prevents the network from relying too heavily on specific neurons, thereby improving generalization.\n",
    "\n",
    "2. **Complex Models**: In deep neural networks with many layers and parameters, dropout can be crucial for preventing overfitting by ensuring that the network does not become too specialized to the training data.\n",
    "\n",
    "3. **Small Datasets**: When working with small datasets, dropout helps in improving the model's ability to generalize by artificially increasing the amount of data through the random dropping of neurons.\n",
    "\n",
    "## Methods for Implementing Dropout\n",
    "\n",
    "Dropout can be implemented in neural networks both from scratch and using high-level libraries. Here, we demonstrate both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b6ab19-4d02-4027-9dc5-495963c01707",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "___\n",
    "___\n",
    "### Readings:\n",
    "- [How Dropout Regularization Mitigates Overfitting in Neural Networks](https://readmedium.com/en/https:/medium.com/data-science-365/how-dropout-regularization-mitigates-overfitting-in-neural-networks-9dcc3e7102ff)\n",
    "- [What is Dropout Regularization method?](https://ai.plainenglish.io/what-is-dropout-regularization-method-1eae267411ef)\n",
    "- [Dropout](https://neuralthreads.medium.com/dropout-regularization-technique-that-clicked-in-geoffrey-hintons-mind-at-a-bank-fa7fa8c5e1fb)\n",
    "- [Types of Regularization in Machine Learning](https://medium.com/towards-data-science/types-of-regularization-in-machine-learning-eb5ce5f9bf50)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c6258-57a0-4043-9ec6-fdd46911598a",
   "metadata": {},
   "source": [
    "## Dropout from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4863317-20d7-4034-a691-405f74898054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.rand(*X.shape) > self.rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        return d_out * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8db33c-f01e-4faf-ad09-d476df8b39ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with dropout:\n",
      " [[0 2 3]\n",
      " [4 0 0]]\n",
      "\n",
      "Backward pass gradients:\n",
      " [[ 0.          0.76743473 -0.46947439]\n",
      " [ 0.54256004 -0.         -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)  \n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "dropout = Dropout(rate=0.5)\n",
    "\n",
    "# Forward pass\n",
    "X_dropout = dropout.forward(X, training=True)\n",
    "print(\"Forward pass with dropout:\\n\", X_dropout)\n",
    "\n",
    "# Backward pass (example gradients from subsequent layers)\n",
    "d_out = np.random.randn(*X.shape)\n",
    "d_X = dropout.backward(d_out)\n",
    "print(\"\\nBackward pass gradients:\\n\", d_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6f126-ac89-44dd-a651-272147812b40",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "## Dropout using `TensorFlow/Keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b7199e-0f6c-405b-abf7-ccd62e234b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b916ed9b-d500-4d49-99df-06fc37a289fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28).astype('float32') / 255\n",
    "x_test = x_test.reshape(-1, 28*28).astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d6d915-4029-4cf0-9d2b-f7f8880570a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with Dropout\n",
    "model = Sequential([\n",
    "    tf.keras.Input(shape=(28*28,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f0c3e9-2551-4862-aeed-fb129c23232a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6748 - loss: 0.9928 - val_accuracy: 0.9410 - val_loss: 0.2082\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8992 - loss: 0.3619 - val_accuracy: 0.9554 - val_loss: 0.1613\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9162 - loss: 0.2933 - val_accuracy: 0.9603 - val_loss: 0.1410\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9284 - loss: 0.2557 - val_accuracy: 0.9619 - val_loss: 0.1341\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9359 - loss: 0.2360 - val_accuracy: 0.9657 - val_loss: 0.1279\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9563 - loss: 0.1466\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01567b5d-6054-4cc9-b76b-14f9bf5b036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1268\n",
      "Test Accuracy: 0.9633\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837504ff-698a-4715-a162-8c97919ed28a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Dropout is a powerful regularization technique used to prevent overfitting in neural networks by randomly dropping a fraction of neurons during training. By applying dropout, we force the network to be less reliant on specific neurons, promoting a more robust model with better generalization.\n",
    "\n",
    "In our exploration of Dropout, we first demonstrated how to implement it manually from scratch. This implementation involves creating a dropout mask and applying it during the forward pass while handling gradients appropriately during the backward pass. This approach provides insight into how dropout functions at a fundamental level.\n",
    "\n",
    "We then utilized TensorFlow/Keras to incorporate dropout into a neural network model with minimal code. TensorFlow's built-in `Dropout` layer simplifies the integration, allowing us to focus on building and training the model efficiently.\n",
    "\n",
    "Both methods show that dropout is beneficial in various scenarios, such as when dealing with large networks, preventing overfitting, or training with limited data. By implementing dropout correctly, we can enhance the performance and generalization capabilities of neural networks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
