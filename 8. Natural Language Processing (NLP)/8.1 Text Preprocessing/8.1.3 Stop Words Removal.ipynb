{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58891592-c955-4987-aa82-df4cab448217",
   "metadata": {},
   "source": [
    "# 8.1.3 Stop Words Removal\n",
    "\n",
    "## Explanation of Stop Words\n",
    "\n",
    "**Stop words** are common words that are filtered out during text processing because they carry little meaningful information. Examples of stop words include \"and\", \"the\", \"is\", \"in\", \"at\", and \"of\". These words are often so frequent that they do not provide significant insight into the content of the text.\n",
    "\n",
    "## Importance of Stop Words Removal in Text Processing\n",
    "\n",
    "Removing stop words is important for several reasons:\n",
    "\n",
    "- **Reduces Noise**: By removing these common words, the focus is shifted to more meaningful terms, which can improve the quality of text analysis.\n",
    "- **Improves Efficiency**: Processing and analyzing text without stop words reduces computational complexity and storage requirements.\n",
    "- **Enhances Accuracy**: Helps in improving the accuracy of text mining tasks, such as topic modeling and sentiment analysis, by emphasizing significant terms.\n",
    "\n",
    "## Methods for Implementing Stop Words Removal\n",
    "\n",
    "### Manual Stop Words List\n",
    "\n",
    "Create a custom list of stop words based on the specific context or language of your text and filter them out during preprocessing.\n",
    "\n",
    "### Using Predefined Lists\n",
    "\n",
    "Leverage predefined stop words lists available in NLP libraries such as NLTK, spaCy, or scikit-learn, which provide commonly used stop words for different languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab026e7a-37db-42b5-a7de-3783f922a2fc",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "- ### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d712aa6-e509-420e-b708-b36ff1fad5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', 'demonstrating', 'stop', 'words', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example text\n",
    "text = \"This is a sample sentence demonstrating stop words removal.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f0739-98a3-48a3-a808-a1f672dc2b5f",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "- ### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916fc91f-7e11-4134-b7ae-cdf9d6a3c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', 'demonstrating', 'stop', 'words', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example text\n",
    "text = \"This is a sample sentence demonstrating stop words removal.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cd830-cc5d-4fe1-b74d-9611be4706e8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Removing stop words from text data is a fundamental step in text preprocessing that helps reduce noise and computational overhead. By focusing on meaningful words, we can improve the performance of various NLP tasks, such as text classification, sentiment analysis, and topic modeling. Both NLTK and spaCy provide convenient methods to efficiently filter out stop words from text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
