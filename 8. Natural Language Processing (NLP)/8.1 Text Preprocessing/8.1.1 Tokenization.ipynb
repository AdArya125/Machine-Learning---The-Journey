{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f4e56d-ba94-4682-a423-68f503228593",
   "metadata": {},
   "source": [
    "# 8.1.1 Tokenization\n",
    "\n",
    "## Explanation of Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units, called tokens. These tokens can be words, phrases, or symbols. Tokenization is a fundamental step in natural language processing (NLP) that helps in converting text into a format that can be analyzed and modeled by algorithms.\n",
    "\n",
    "## Benefits and Use Cases of Tokenization\n",
    "\n",
    "- **Simplifies Text Processing:** Tokenization converts text into manageable pieces, making it easier to analyze.\n",
    "- **Facilitates Text Analysis:** Helps in tasks such as sentiment analysis, text classification, and named entity recognition.\n",
    "- **Enables Feature Extraction:** Converts text into features that can be used in machine learning models.\n",
    "- **Improves Text Understanding:** Helps algorithms understand the structure and meaning of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb06e8ba-a652-42f2-aed7-a251af7f9910",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### Readings:\n",
    "- [Tokenization â€” A complete guide](https://medium.com/@utkarsh.kant/tokenization-a-complete-guide-3f2dd56c0682)\n",
    "- [Tokenization in NLP : All you need to know](https://medium.com/@abdallahashraf90x/tokenization-in-nlp-all-you-need-to-know-45c00cfa2df7)\n",
    "- [All about Tokenizers](https://vidhi-chugh.medium.com/all-about-tokenizers-fe92443e2ad)\n",
    "- [What is Tokenization?](https://www.datacamp.com/blog/what-is-tokenization)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41cb85-8204-4585-bbfd-16d3d98a8d02",
   "metadata": {},
   "source": [
    "## Methods for Implementing Tokenization\n",
    "\n",
    "### 1. Word Tokenization\n",
    "\n",
    "This method involves splitting text into individual words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156531dd-56ab-44ca-8dbf-46bddf75ade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'Tokenization', 'is', 'essential', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hello, world! Tokenization is essential in NLP.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df018e8f-7808-4e53-8cfe-5ebe474e4b58",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### 2. Sentence Tokenization\n",
    "This method splits text into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5295ca17-4d03-42a8-8413-5dc15131f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, world!', 'Tokenization is essential in NLP.', \"Let's understand it better.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Hello, world! Tokenization is essential in NLP. Let's understand it better.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf385c9-047b-4fb9-822a-9921073883f5",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### 3. Tokenization Using Regular Expressions\n",
    "Custom tokenization patterns can be created using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afef0864-056b-433d-8504-e19749039f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'Tokenization', 'is', 'essential', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world! Tokenization is essential in NLP.\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7564e1a-d571-4885-a040-7448e08093d9",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Tokenization is a crucial preprocessing step in natural language processing that simplifies and prepares text for further analysis. By breaking down text into manageable units such as words or sentences, tokenization facilitates a range of NLP tasks, from text classification to sentiment analysis. Implementing tokenization effectively using libraries like NLTK, spaCy, or custom regular expressions allows for better handling and understanding of textual data. Mastery of tokenization techniques is essential for building robust NLP models and applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
