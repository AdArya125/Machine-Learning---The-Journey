{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afa6e6d-d14a-41f8-bbdf-8cf1ffb329a9",
   "metadata": {},
   "source": [
    "# 8.2.3 Word2Vec\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Word2Vec** is a popular word embedding technique developed by researchers at Google. It represents words in a continuous vector space where semantically similar words are mapped to nearby points. This model can capture linguistic relationships between words, such as synonyms and analogies. **Word2Vec** uses neural networks to learn word representations from large text corpora. There are two main architectures for training Word2Vec:\n",
    "\n",
    "- **Continuous Bag-of-Words (CBOW)**: Predicts the current word based on the context (surrounding words).\n",
    "- **Skip-gram**: Predicts the context (surrounding words) based on the current word.\n",
    "\n",
    "## Benefits of Word2Vec\n",
    "- **Captures Semantics**: Effectively captures the meaning of words and their relationships.\n",
    "- **Efficient**: Can handle large vocabularies and corpora.\n",
    "- **Versatile**: Useful in various NLP tasks, such as text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef3685-07f0-4aa8-94b0-6614bfbcdeb7",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### **Readings**:\n",
    "- [Word2Vec For Word Embeddings -A Beginner’s Guide](https://www.analyticsvidhya.com/blog/2021/07/word2vec-for-word-embeddings-a-beginners-guide/)\n",
    "- [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)\n",
    "- [Word2Vec Explained](https://readmedium.com/en/https:/towardsdatascience.com/word2vec-explained-49c52b4ccb71)\n",
    "- [A Dummy’s Guide to Word2Vec](https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673)\n",
    "- [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42dfb3c8-f4b9-4513-ac4b-2722b60fd5c2",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5928ed54-b449-44ac-a03f-b1ec7973103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW model - Words similar to 'king':\n",
      "\n",
      "('Birmingham', 0.9642075300216675)\n",
      "('master', 0.9562150239944458)\n",
      "('graceful', 0.9525375366210938)\n",
      "('rousing', 0.9525105953216553)\n",
      "('Vienna', 0.9507602453231812)\n",
      "('singing', 0.9505473971366882)\n",
      "('tail', 0.9495171904563904)\n",
      "('guitar', 0.9481744170188904)\n",
      "('skin', 0.947297990322113)\n",
      "('Model', 0.9471037983894348)\n",
      "\n",
      "Skip-gram model - Words similar to 'king':\n",
      "\n",
      "('aunt', 0.9303045272827148)\n",
      "('tease', 0.9267407655715942)\n",
      "('comrades', 0.9239540100097656)\n",
      "('Alvin', 0.9182977080345154)\n",
      "('adventurous', 0.9182565808296204)\n",
      "('numbered', 0.9167513847351074)\n",
      "('embarrassed', 0.9160632491111755)\n",
      "('Dick', 0.9150828123092651)\n",
      "('Ritter', 0.9150470495223999)\n",
      "('followers', 0.9146966338157654)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Load the Brown corpus\n",
    "sentences = brown.sents()\n",
    "\n",
    "# Train the Word2Vec model using the CBOW architecture\n",
    "model_cbow = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "\n",
    "# Train the Word2Vec model using the Skip-gram architecture\n",
    "model_skipgram = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4, sg=1)\n",
    "\n",
    "# Example: Find similar words to 'king'\n",
    "similar_words_cbow = model_cbow.wv.most_similar('king')\n",
    "similar_words_skipgram = model_skipgram.wv.most_similar('king')\n",
    "\n",
    "print(\"CBOW model - Words similar to 'king':\\n\")\n",
    "for word in similar_words_cbow:\n",
    "    print(word)\n",
    "print(\"\\nSkip-gram model - Words similar to 'king':\\n\")\n",
    "for word in similar_words_skipgram:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554ec20-d0e2-4792-b216-9d429ca7b8d8",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "## Conclusion\n",
    "\n",
    "Word2Vec is a powerful tool for transforming text into meaningful vector representations. By capturing the semantic relationships between words, it enables more sophisticated natural language processing tasks. The CBOW and Skip-gram architectures offer flexibility in training models based on different context prediction approaches. Word2Vec's efficiency and versatility make it a valuable asset in various applications, from text classification to machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c531e0-0b28-4302-8e8e-4f1b1b457255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
