{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa67885-0d86-4f32-ac6f-9e53699595fb",
   "metadata": {},
   "source": [
    "# Introduction to Text Data Preprocessing\n",
    "\n",
    "Text data preprocessing is a fundamental step in preparing textual data for machine learning models and natural language processing (NLP) tasks. Raw text data often contains various inconsistencies, redundancies, and noise that can adversely affect the performance of models. Preprocessing the text data ensures that it is clean, consistent, and in a suitable format for analysis.\n",
    "\n",
    "The key steps in text data preprocessing include:\n",
    "\n",
    "1. **Tokenization**: Splitting text into individual words or tokens.\n",
    "2. **Stemming**: Reducing words to their root form by removing suffixes.\n",
    "3. **Lemmatization**: Converting words to their base or dictionary form.\n",
    "\n",
    "These steps help in standardizing the text data, reducing its dimensionality, and making it more manageable for algorithms to process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a9bb73-78b3-4755-bcb0-6e57b514441c",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Consider the following raw text data:\n",
    "\n",
    "**\"The quick brown fox jumps over the lazy dog. The dogs are barking loudly in the yard.\"**\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units, called tokens. Tokens can be words, phrases, or even characters. In this example, we will tokenize the text into words:\n",
    "\n",
    "**['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dogs', 'are', 'barking', 'loudly', 'in', 'the', 'yard', '.']**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b5b1d-0bcd-47d4-a5bc-6628cedc6a1b",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing words to their root form. This helps in standardizing words with the same base meaning but different variations. Using a stemming algorithm like PorterStemmer, the tokens are reduced to their root forms:\n",
    "\n",
    "**['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.', 'the', 'dog', 'are', 'bark', 'loudli', 'in', 'the', 'yard', '.']**\n",
    "\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "Lemmatization is **similar to stemming** but aims to reduce words to their dictionary form. It uses the context and part of speech to accurately convert words to their base form. Using a lemmatizer, the tokens are converted to:\n",
    "\n",
    "**['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.', 'the', 'dog', 'be', 'bark', 'loudly', 'in', 'the', 'yard', '.']**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e83a6bef-5f4c-443b-ac9e-dc0821f45c04",
   "metadata": {},
   "source": [
    "!pip install wordcloud\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ec4ce-2d88-41c6-8456-83180212820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62f24a-81d4-46d2-8d43-796e34f6600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bdfe5f-6171-405d-a0a5-4117fd6edb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data\n",
    "data = {\n",
    "    'Text': [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"The dogs are barking loudly in the yard.\",\n",
    "        \"Running is a great exercise to keep yourself fit.\",\n",
    "        \"Cats are wonderful companions for many people.\",\n",
    "        \"I love programming and solving complex problems.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original text data\n",
    "print(\"Original Text Data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3604b8-c731-4732-bb38-010112f0cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "df['Tokenized'] = df['Text'].apply(word_tokenize)\n",
    "print(df['Tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cbc608-e6cc-450f-9935-8b4cde9d2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['Stemmed'] = df['Tokenized'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "print(df['Stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f8fcf-5cbb-4a5a-81d5-c43e59ec45b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Lemmatized'] = df['Tokenized'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print(df['Lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9f339-e8f0-43a1-a707-31efe6a50547",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Visualizations\n",
    "\n",
    "To better understand the impact of these preprocessing steps, we can visualize the text data before and after preprocessing. ***Word clouds*** are a useful tool for this purpose, as they display the **most frequent words** in the text.\n",
    "\n",
    "By understanding and applying these preprocessing techniques, you can improve the quality and effectiveness of your text-based models, ensuring that the data is in a suitable format for further analysis and machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1db2e-a899-469f-b26a-a4f0fedf61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of preprocessing\n",
    "def plot_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n",
    "    print(f\"{title}\\n\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc682e6-ee52-49d2-adac-936c6d64763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tokenized words for visualization\n",
    "original_text = df['Text'].str.cat(sep=' ')\n",
    "tokenized_text = df['Tokenized'].sum()\n",
    "stemmed_text = df['Stemmed'].sum()\n",
    "lemmatized_text = df['Lemmatized'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb2eb0-4145-49ca-9ad5-b4b568227b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot word clouds\n",
    "plot_wordcloud(original_text.split(), 'Original Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5876d5-d4bc-4ea9-8589-69a998e62f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_wordcloud(tokenized_text, 'Tokenized Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10756dbc-0518-496e-84ff-d3151a672061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_wordcloud(stemmed_text, 'Stemmed Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0d904-ee7d-45f6-8f90-eaf6796e00d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_wordcloud(lemmatized_text, 'Lemmatized Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7140e5-2906-42fb-a9ca-f9b4640afb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the processed text data\n",
    "print(\"\\nOriginal Text Data:\")\n",
    "print(df['Text'])\n",
    "\n",
    "print(\"\\nTokenized Text Data:\")\n",
    "print(df['Tokenized'])\n",
    "\n",
    "print(\"\\nStemmed Text Data:\")\n",
    "print(df['Stemmed'])\n",
    "\n",
    "print(\"\\nLemmatized Text Data:\")\n",
    "print(df['Lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19017219-3063-4a0c-be19-e672ca0c44ed",
   "metadata": {},
   "source": [
    "## Conclusion\r\n",
    "\r\n",
    "Text data preprocessing is an essential step in the workflow of natural language processing (NLP) and machine learning. By systematically cleaning and transforming raw text data, we can significantly enhance the performance and accuracy of models. The primary techniques of tokenization, stemming, and lemmatization each play a crucial role in this process:\r\n",
    "\r\n",
    "- **Tokenization** breaks down the text into manageable units, allowing for more precise analysis and manipulation.\r\n",
    "- **Stemming** reduces words to their root forms, helping in the standardization of textual data.\r\n",
    "- **Lemmatization** converts words to their base or dictionary form, ensuring that words with similar meanings are treated equivalently.\r\n",
    "\r\n",
    "These preprocessing steps not only make the data more uniform but also reduce the complexity of the text, making it more suitable for analysis and modeling. By visualizing the text data before and after preprocessing, we can better appreciate the impact of these techniques and understand how they contribute to the overall effectiveness of text-based models.\r\n",
    "\r\n",
    "In conclusion, mastering these preprocessing techniques is crucial for anyone working with text data, as it lays the foundation for building robust and efficient NLP and machine learnig models.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
